<p>Marterials will be available on http://www.mlss2014.com/materials.html</p>

<ol>
  <li>Intro to ML — Zico Kolter
Mainly 3 aspects
1) Supervised Learning : Regression
2) Non-Linear Regression, overfitting and model selection
3) Supervised Learning : Classification</li>
</ol>

<p>Zico just told very basic information, from linear regression to classification.</p>

<p>From basic introduction to ML, and pull the canonical ml problem for best model and min loss function. Different kind of functions used in ML, and the overfitting and regularisation problem =&gt; model complexity. About why and how we should use regularisation, even I knew this part before but I havent look deep into it, so for this time, I had a chance to learn how it works.</p>

<p>Next part is classification problems for how we set our boundary and choose which kind loss function and how to write it.For example SVM is with hinge loss and linear prediction.</p>

<p>The last thing he mentioned is stochastic gradient descent, which is for big data calculation, we perform gradient updates for one example at a time.</p>

<ol>
  <li>Scaling Machine Learning — Alex Smola</li>
</ol>

<p>Smola’s lecture is mainly about the hardware and computing big data
For now, we can see users produce data everyday, and if we wanna do analysis on them , we should have a high quality computing ability, this is not only algorithm problem or hardware, it’s a combination problem.</p>

<p>1) RAID Redundant array of inexpensive disk</p>

<p>2) Distributed replicated file system</p>

<p>3) Google File System / Hadoop FS &amp; HDFS link:https://github.com/apache/hadoop-hdfs</p>

<p>4) MapReduce (Map Combine Reduve)
here he provided the idea: use MapReduce to do the gradient descent when we deal with big data machine learning instead of using the stochastic gradient descent
* compute gradient
**on each data point via Map(i, data)
**sum gradient via Reduce(coordinate)
*perform update step
For this part I just looked some info about the mincepie which is a lightweight map reduce in python link here:https://github.com/Yangqing/mincepie , and I think by this way, it can fast our machine learning system. I will follow this topic further.</p>

<p>5) Dryad link:https://github.com/MicrosoftResearch/Dryad
6) S4 link:https://github.com/s4/s4</p>

<p>7) spark link https://github.com/apache/spark
spark is really faster than map reduce, and it is called lightning fast cluster computing.</p>

<ol>
  <li>Bayesian Optimisation meets Deep Learning — de Freitas</li>
</ol>

<p>Actually for this part I didn’t make sense of too much about it, cuz i didn’t get to know this kind of things before. My friend Zoy told me it’s for find the best parameter of the function, for example, given a function with consecutive values we wanna find a min or max value, it’s called black box optimisation, it will help determine which point we will explore next. Ive download a paper of this. and I would like to see it later.</p>

<p>Thompson sampling and Entropy search profolios</p>

<ol>
  <li>Deep Learning — Quoc Le</li>
</ol>

<p>Quoc Le is from Google who’s was in Andrew Ng Team and Google Brain Project.
His lecture is mainly based on 3 aspects
1) Linear Classification
2) Neural Network
3) Back prop Algorithm</p>

<p>In his lecture, the main structure is to start from introduce the linear classification which is exactly the logistic regression.
for finding a best parameter, uses arg min function I wonder if it is for bayesian optimisation, also he uses the stochastic gradient descent.</p>

<p>that while process will be like
*perform feed forward
**step to compute h 1 to n
*for output layer compute
*perform backdrop pass</p>

<ol>
  <li>Parameter Server Tutorial
Mu Li from Baidu</li>
</ol>

<p>Actually his part is kind of  similar to Smola.
and I am going through the jet lag, so I just fallllllll asleep.</p>
