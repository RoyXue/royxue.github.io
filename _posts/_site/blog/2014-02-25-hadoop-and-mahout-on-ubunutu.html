<h1 id="hadoop-220---mahout--ubuntu-1204-">Hadoop 2.2.0 伪分布式环境 和 Mahout 在 Ubuntu 12.04 配置过程</h1>

<hr />

<h2 id="ubuntu-1204-">1.Ubuntu 12.04 安装</h2>

<p>下载Ubuntu 12.04 LTS 稳定版安装即可.如果可以的话推荐安装32位版本,因为Hadoop只有32位的包,64位运行需要自己编译一下.</p>

<h2 id="section">2.用户组设置</h2>

<h4 id="hadoop">2.1 创建hadoop用户组和用户</h4>

<pre><code>&lt;code&gt;sudo addgroup hadoop
sudo adduser -ingroup hadoop hadoop
&lt;/code&gt;
</code></pre>

<h4 id="hadoop-1">2.2 为hadoop用户添加对应权限</h4>

<pre><code>&lt;code&gt;sudo gedit /etc/sudoers #打开sudoers文件
&lt;/code&gt;
</code></pre>

<p>在文件中root ALL=(ALL:ALL) ALL下添加hadoop ALL=(ALL:ALL) ALL</p>

<h2 id="jdk">3.JDK的安装</h2>

<h4 id="jdk-1">3.1 JDK下载解压</h4>

<p>linux下通过apt-get可以安装open-jdk和sun-jdk两种,如果通过这种方法安装推荐使用sun-jdk. 我是使用Oracle的jdk进行安装.</p>

<p>下载链接 <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html">http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html</a></p>

<p>下载之后cd到Downloads文件夹</p>

<pre><code>&lt;code&gt;sudo tar -zxf java文件夹名
sudo mv /java /usr
&lt;/code&gt;
</code></pre>

<h4 id="section-1">3.2 设置环境变量</h4>

<pre><code>&lt;code&gt;sudo gedit /etc/profile
&lt;/code&gt;
</code></pre>

<p>在其中加入</p>

<pre><code>&lt;code&gt;# for java
export JAVA_HOME=/usr/java
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
&lt;/code&gt;
</code></pre>

<p>保存退出</p>

<pre><code>&lt;code&gt;source /etc/profile
&lt;/code&gt;
</code></pre>

<h4 id="jdk-2">3.3 JDK验证</h4>

<pre><code>&lt;code&gt;java -version
&lt;/code&gt;
</code></pre>

<p>正确显示说明JDK已经正确安装</p>

<h2 id="ssh">4. ssh服务的安装</h2>

<h4 id="ssh-1">4.1 ssh安装</h4>

<pre><code>&lt;code&gt;sudo apt-get install ssh openssh-server
&lt;/code&gt;
</code></pre>

<h4 id="section-2">4.2 无密码登录配置</h4>

<p>切换用户</p>

<pre><code>&lt;code&gt;su hadoop
&lt;/code&gt;
</code></pre>

<p>默认采用ssh的rsa方式生成密钥</p>

<pre><code>ssh-keygen -t rsa -P ""
cd ~/.ssh
cat id_rsa.pub &gt;&gt; authorized_keys
</code></pre>

<p>完成此步骤即完成了无密码验证</p>

<h4 id="localhost">4.3 登录localhost和退出</h4>

<pre><code>&lt;code&gt;ssh localhost
exit
&lt;/code&gt;
</code></pre>

<h2 id="hadoop-2">5. hadoop的安装和配置</h2>

<h4 id="hadoop-3">5.1 hadoop下载解压</h4>

<p>在这里用的是hadoop 2.2.0 32位版本<a href="http://hadoop.apache.org/#Download+Hadoop">http://hadoop.apache.org/#Download+Hadoop</a></p>

<p>64位重新编译版<a href="http://pan.baidu.com/s/11SPwd">http://pan.baidu.com/s/11SPwd</a></p>

<p>照例解压并且移动到/usr改名为hadoop,并且给hadoop文件夹分配权限</p>

<pre><code>&lt;code&gt;sudo chown -R hadoop:hadoop hadoop
&lt;/code&gt;
</code></pre>

<p>配置hadoop路径</p>

<pre><code>&lt;code&gt;sudo gedit /etc/profile
&lt;/code&gt;
</code></pre>

<p>加入</p>

<pre><code>&lt;code&gt;#set hadoop env
export HADOOP_HOME=/usr/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
&lt;/code&gt;
</code></pre>

<p>使配置生效</p>

<pre><code>&lt;code&gt;source /etc/profile
&lt;/code&gt;
</code></pre>

<h4 id="hadoop-4">5.2 hadoop基础配置</h4>

<p>5.2.1 配置conf/hadoop-env.sh</p>

<p>找到#export JAVA_HOME=…,去掉#，然后加上本机jdk的路径）</p>

<pre><code>&lt;code&gt;export JAVA_HOME=/usr/java
&lt;/code&gt;
</code></pre>

<p>5.2.2 配置 core-site.html</p>

<pre><code>&lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/home/hadoop/hadoop_tmp&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>5.2.5 配置hdfs-site.xml</p>

<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;/home/namenode&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;/home/datanode&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>5.2.4 配置mapred-site.xml</p>

<pre><code>&lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>5.2.5 yarn-site.xml</p>

<pre><code> &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<h4 id="hadoop-">5.3 Hadoop 启动运行</h4>

<p>启动的文件都是 sbin下，bin下的都是命令。 使用命令cd $HADOOP_HOME切换到该安装目录下 首先格式化 namenode</p>

<pre><code>&lt;code&gt;bin/hdfs namenode -format
&lt;/code&gt;
</code></pre>

<p>确定不报错，且出现</p>

<p>SHUTDOWN_MSG: Shutting down NameNode at startos/localhost</p>

<p>启动namenode</p>

<pre><code>&lt;code&gt;sbin/hadoop-daemon.sh start namenode
sbin/hadoop-daemon.sh start datanode
&lt;/code&gt;
</code></pre>

<p>运行测试</p>

<p>1 jps 出现： 12935 NameNode 5309 Jps 13012 DataNode</p>

<p>证明启动成功，如果没有出现DataNode或者NameNode，证明启动没有成功，可以查看hadoop安装目录下的logs下的日志记录。 可以使用sbin/hadoop-daemon.sh stop datanode（或namenode）来关闭。</p>

<p>启动Manage管理</p>

<pre><code>&lt;code&gt;sbin/yarn-daemon.sh start resourcemanager
sbin/yarn-daemon.sh start nodemanager
&lt;/code&gt;
</code></pre>

<p>运行测试</p>

<p>1 jps 出现：</p>

<pre><code>&lt;code&gt;13338 NodeManager
13111 ResourceManager
12935 NameNode
5309 Jps
13012 DataNode
&lt;/code&gt;
</code></pre>

<p>证明启动成功 同时也可以使用yarn-daemon.sh stop resourcemanager（nodemanager）来关闭。</p>

<p>如果没有单独配置yarn-site.xml中的yarn.resourcemanager.webapp.address，默认的端口8088 访问 http://127.0.0.1:8088/ 可以访问hadoop管理页面</p>

<p>如果没有单独配置 hdfs-site.xml中的dfs.namenode.http-address,默认端口50070 http://127.0.0.1:50070 可以访问namenode节点信息。</p>

<h2 id="mahout">6.Mahout的配置</h2>

<h4 id="mahout-1">6.1 Mahout的下载解压</h4>

<p>Mahout的下载地址<a href="http://mahout.apache.org/general/downloads.html">http://mahout.apache.org/general/downloads.html</a></p>

<p>解压并移动到/usr文件夹,并配置环境变量
sudo gedit /etc/profile 在里面加入</p>

<pre><code>&lt;code&gt;#set mahout env
export HADOOP_CONF_DIR=/usr/hadoop/etc/hadoop
export MAHOUT_HOME=/usr/mahout-distribution-0.4
export PATH=$PATH:$MAHOUT_HOME/bin
&lt;/code&gt;
</code></pre>

<p>是配置生效</p>

<pre><code>&lt;code&gt;source /etc/profile
&lt;/code&gt;
</code></pre>

<p>输入mahout检查是否完成安装</p>

<h4 id="mahoutk-means">6.2 mahout的K-Means算法测试</h4>

<p>6.2.1 下载数据集synthetic_control.data 地址:<a href="http://archive.ics.uci.edu/ml/databases/synthetic_control/synthetic_control.data">http://archive.ics.uci.edu/ml/databases/synthetic_control/synthetic_control.data</a></p>

<p>6.2.2 创建测试目录testdata,并把数据导入到这个test目录中(这里的目录的名字只能是testdata)</p>

<pre><code>&lt;code&gt;hdfs dfs -mkdir -p /user/hadoop/testdata
hdfs dfs -put /home/synthetic_control.data /user/hadoop/testdata/
&lt;/code&gt;
</code></pre>

<p>6.2.3 运行kmeans算法</p>

<pre><code>&lt;code&gt;mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job
&lt;/code&gt;
</code></pre>

<p>运行大概五分钟左右就可以看到聚类的结果,至此,Mahout的安装配置和测试工作完成.</p>
